{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0gfAbD1BRax"
   },
   "source": [
    "# Assignment 5 #\n",
    "### Due: Friday, November 17th to be submitted via Canvas by 11:59 pm ###\n",
    "### Total points: **65** ###\n",
    "\n",
    "1. Homework Group - 68\n",
    "2. Student Names - Anmol Agrawal (aa96674)\n",
    "                   Aishwarya Parida (ap63595)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omOSpxMmBTQE"
   },
   "source": [
    "# Q1: Support Vector Machines (10 points)\n",
    "\n",
    "In this question, we will explore support vector machines for the Spam Base dataset from the UCI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0GbdHYxjLiok"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tVj75ETFMaUJ"
   },
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "26ixmQ9qT6z3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/anmolagrawal/anaconda3/lib/python3.10/site-packages (0.0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DlQyfxg0YuB0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: Classifying Email as Spam or Non-Spam\n",
      "Number of instances: 4601\n",
      "Number of features: 57\n",
      "0                 word_freq_make\n",
      "1              word_freq_address\n",
      "2                  word_freq_all\n",
      "3                   word_freq_3d\n",
      "4                  word_freq_our\n",
      "5                 word_freq_over\n",
      "6               word_freq_remove\n",
      "7             word_freq_internet\n",
      "8                word_freq_order\n",
      "9                 word_freq_mail\n",
      "10             word_freq_receive\n",
      "11                word_freq_will\n",
      "12              word_freq_people\n",
      "13              word_freq_report\n",
      "14           word_freq_addresses\n",
      "15                word_freq_free\n",
      "16            word_freq_business\n",
      "17               word_freq_email\n",
      "18                 word_freq_you\n",
      "19              word_freq_credit\n",
      "20                word_freq_your\n",
      "21                word_freq_font\n",
      "22                 word_freq_000\n",
      "23               word_freq_money\n",
      "24                  word_freq_hp\n",
      "25                 word_freq_hpl\n",
      "26              word_freq_george\n",
      "27                 word_freq_650\n",
      "28                 word_freq_lab\n",
      "29                word_freq_labs\n",
      "30              word_freq_telnet\n",
      "31                 word_freq_857\n",
      "32                word_freq_data\n",
      "33                 word_freq_415\n",
      "34                  word_freq_85\n",
      "35          word_freq_technology\n",
      "36                word_freq_1999\n",
      "37               word_freq_parts\n",
      "38                  word_freq_pm\n",
      "39              word_freq_direct\n",
      "40                  word_freq_cs\n",
      "41             word_freq_meeting\n",
      "42            word_freq_original\n",
      "43             word_freq_project\n",
      "44                  word_freq_re\n",
      "45                 word_freq_edu\n",
      "46               word_freq_table\n",
      "47          word_freq_conference\n",
      "48                   char_freq_;\n",
      "49                   char_freq_(\n",
      "50                   char_freq_[\n",
      "51                   char_freq_!\n",
      "52                   char_freq_$\n",
      "53                   char_freq_#\n",
      "54    capital_run_length_average\n",
      "55    capital_run_length_longest\n",
      "56      capital_run_length_total\n",
      "57                         Class\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "# fetch dataset\n",
    "spambase = fetch_ucirepo(id=94)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = spambase.data.features\n",
    "y = spambase.data.targets\n",
    "print(\"Abstract:\", spambase.metadata['abstract'])\n",
    "print(\"Number of instances:\", spambase.metadata['num_instances'])\n",
    "print(\"Number of features:\", spambase.metadata['num_features'])\n",
    "print(spambase.variables['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lmKGx3T8ZXci"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "0             0.00            0.00  ...                   0.0         0.00   \n",
       "1             0.00            0.94  ...                   0.0         0.00   \n",
       "2             0.64            0.25  ...                   0.0         0.01   \n",
       "3             0.31            0.63  ...                   0.0         0.00   \n",
       "4             0.31            0.63  ...                   0.0         0.00   \n",
       "\n",
       "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.000          0.0        0.778        0.000        0.000   \n",
       "1        0.132          0.0        0.372        0.180        0.048   \n",
       "2        0.143          0.0        0.276        0.184        0.010   \n",
       "3        0.137          0.0        0.137        0.000        0.000   \n",
       "4        0.135          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                       278  \n",
       "1                      1028  \n",
       "2                      2259  \n",
       "3                       191  \n",
       "4                       191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tukYZVbDZsG-"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "y = y.to_numpy().squeeze()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jdODxjI41xm"
   },
   "source": [
    "a. (5 points) Implement the following function to train SVMs with a specified kernel type, hyper-parameter search space, and random state on the Spam Base dataset. Do hyper-parameter search over $C$ using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), setting the number of folds to 5. After finding the best C, please use it to train the final model and return both the final model and the best C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K90sxpVK7mED"
   },
   "outputs": [],
   "source": [
    "def search_best_svm(kernel, C_search_space, random_state):\n",
    "    best_score = -np.inf\n",
    "    best_C=None\n",
    "    for C in C_search_space:\n",
    "        # Initialize an SVM classifier with the specified kernel type, C value, and random state\n",
    "        ### START CODE ###\n",
    "        svm = SVC(kernel=kernel, C=C, random_state=random_state)\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Evaluate accuracy scores using 5-fold cross-validation scores\n",
    "        ### START CODE ###\n",
    "        scores = cross_val_score(svm, X_train, y_train, cv=5)\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Compute the average score and compare with the current best score to update the best C\n",
    "        ### START CODE ###\n",
    "        average_score = np.mean(scores)\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_C = C\n",
    "        ### END CODE ###\n",
    "        #print(f\"C: {C} Avg Cross Val Score: {np.round(score, 4)}\")\n",
    "        print(f\"C: {C}, Avg Cross Val Score: {average_score:.4f}\")\n",
    "\n",
    "    #print(f\"Best C: {best_C}\")\n",
    "    print(f\"Best C: {best_C}, Best Score: {best_score:.4f}\")\n",
    "\n",
    "    # Initialize the model using the specified kernel type, best C, and random state;\n",
    "    # and then fit the model using training set\n",
    "    ### START CODE ###\n",
    "    model = SVC(kernel=kernel, C=best_C, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    ### END CODE ###\n",
    "    return model, best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ_MU9Fl6EIn"
   },
   "source": [
    "b. (3 points) Run the function you implemented above to train SVMs with the search space of $C$ being [$0.1, 1, 10, 100$], random state set to 42, with the following three popular kernels: (i) linear (ii) polynomial (iii) RBF (Gaussian). Evaluate your final models on the test set and report their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, Avg Cross Val Score: 0.9214\n",
      "C: 1, Avg Cross Val Score: 0.9264\n",
      "C: 10, Avg Cross Val Score: 0.9283\n",
      "C: 100, Avg Cross Val Score: 0.9268\n",
      "Best C: 10, Best Score: 0.9283\n",
      "The best C for the linear kernel is 10 with an accuracy of 92.83% on the test set.\n",
      "\n",
      "C: 0.1, Avg Cross Val Score: 0.6895\n",
      "C: 1, Avg Cross Val Score: 0.7594\n",
      "C: 10, Avg Cross Val Score: 0.8428\n",
      "C: 100, Avg Cross Val Score: 0.8957\n",
      "Best C: 100, Best Score: 0.8957\n",
      "The best C for the poly kernel is 100 with an accuracy of 91.25% on the test set.\n",
      "\n",
      "C: 0.1, Avg Cross Val Score: 0.8960\n",
      "C: 1, Avg Cross Val Score: 0.9243\n",
      "C: 10, Avg Cross Val Score: 0.9243\n",
      "C: 100, Avg Cross Val Score: 0.9123\n",
      "Best C: 10, Best Score: 0.9243\n",
      "The best C for the rbf kernel is 10 with an accuracy of 93.54% on the test set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C_search_space = [0.1, 1, 10, 100]\n",
    "random_state = 42\n",
    "best_models = {}\n",
    "accuracies = {}\n",
    "\n",
    "for kernel in ['linear', 'poly', 'rbf']:\n",
    "    model, best_C = search_best_svm(kernel, C_search_space, random_state)\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[kernel] = model\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the accuracy\n",
    "    accuracies[kernel] = accuracy\n",
    "    \n",
    "    print(f\"The best C for the {kernel} kernel is {best_C} with an accuracy of {accuracy*100:.2f}% on the test set.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCxwqenL6nZ4"
   },
   "source": [
    "c. (2 points) Train a logistic regression model using the training set. Compare its performance with that of the SVMs trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from Logistic Regression: 92.23%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94      1089\n",
      "           1       0.93      0.88      0.90       752\n",
      "\n",
      "    accuracy                           0.92      1841\n",
      "   macro avg       0.92      0.92      0.92      1841\n",
      "weighted avg       0.92      0.92      0.92      1841\n",
      "\n",
      "Comparing with SVMs:\n",
      "Linear SVM Test Accuracy : 92.83%\n",
      "Polynomial SVM Test Accuracy : 91.25%\n",
      "Radial Basis Function SVM Test Accuracy : 93.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f'Accuracy from Logistic Regression: {accuracy*100:.2f}%')\n",
    "print(f'Classification Report:\\n{report}')\n",
    "\n",
    "\n",
    "print(f\"Comparing with SVMs:\")\n",
    "print(f\"Linear SVM Test Accuracy : {accuracies['linear']*100:.2f}%\")\n",
    "print(f\"Polynomial SVM Test Accuracy : {accuracies['poly']*100:.2f}%\")\n",
    "print(f\"Radial Basis Function SVM Test Accuracy : {accuracies['rbf']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, we observe that Radial Basis Function performs with the best accuracy (93.54%).The RBF kernel is useful in SVM when dealing with non-linearly separable data which suggests that there may be a non linear distribution in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc9ca1b5"
   },
   "source": [
    "# Question 2 : Ensemble Methods for Classification (25 pts)\n",
    "\n",
    "In this question, we will compare the performances of different ensemble methods for classification problems: [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) classifiers.\n",
    "\n",
    "We will look at the [GiveMeSomeCredit](https://www.kaggle.com/c/GiveMeSomeCredit) dataset for this question. The dataset is extremely large so for this question we will only consider a subset which has been provided along with the notebook for this assignment. The dataset has already been split into train and test sets.\n",
    "\n",
    "The task is to predict the probability that someone will experience financial distress in the next two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 6176,
     "status": "ok",
     "timestamp": 1699650398287,
     "user": {
      "displayName": "Song Wang",
      "userId": "14539824110514115974"
     },
     "user_tz": 360
    },
    "id": "a93b913f",
    "outputId": "789864e1-5a42-4df6-a7cd-4cea1102cde2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-dc578b7f-3ab1-4613-95e2-300439976811\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-dc578b7f-3ab1-4613-95e2-300439976811\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving hw5_data.csv to hw5_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Only use this code block if you are using Google Colab.\n",
    "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
    "from google.colab import files\n",
    "\n",
    "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
    "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.2-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/anmolagrawal/anaconda3/lib/python3.10/site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in /Users/anmolagrawal/anaconda3/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1a2bac93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.571373</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430620</td>\n",
       "      <td>9274.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257380</td>\n",
       "      <td>5656.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.299270</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.114575</td>\n",
       "      <td>4747.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.032165</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308326</td>\n",
       "      <td>8490.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050591</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.862627</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0                 0                              0.571373   66   \n",
       "1                 0                              0.233999   56   \n",
       "2                 0                              0.299270   33   \n",
       "3                 0                              0.032165   41   \n",
       "4                 0                              0.050591   36   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                     0   0.430620         9274.0   \n",
       "1                                     0   0.257380         5656.0   \n",
       "2                                     0   0.114575         4747.0   \n",
       "3                                     0   0.308326         8490.0   \n",
       "4                                     0   0.862627         3333.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               10                        0   \n",
       "1                               12                        0   \n",
       "2                                8                        0   \n",
       "3                                8                        0   \n",
       "4                                8                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             1                                     0   \n",
       "1                             0                                     0   \n",
       "2                             0                                     0   \n",
       "3                             1                                     0   \n",
       "4                             2                                     0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 3.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('hw5_data.csv')\n",
    "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fb34060d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3750, 10) (3750,)\n",
      "test: (1250, 10) (1250,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = data['SeriousDlqin2yrs']\n",
    "X = data.drop(['SeriousDlqin2yrs'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 7)\n",
    "\n",
    "print('train:',X_train.shape, y_train.shape)\n",
    "print('test:',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "90d16082"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from time import time\n",
    "import xgboost\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "420aeb16"
   },
   "outputs": [],
   "source": [
    "columns_list = list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f40f9107"
   },
   "source": [
    "a. (2.5 pts) Fit a Decision Tree Classifier with random_state = 14 for this classification problem. Report the accuracy_score and roc_auc_score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "45de59e6"
   },
   "outputs": [],
   "source": [
    "def fit_classifier(clf, X_train, y_train):\n",
    "    # Fit the classifier on the training set\n",
    "  ### START CODE ###\n",
    "    clf.fit(X_train, y_train)\n",
    "  ### END CODE ###\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yZfiYRKtHJQ4"
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf, X_test, y_test):\n",
    "  # Compute the accuracy_score, and roc_auc_score on the test set\n",
    "  ### START CODE ###\n",
    "  y_pred = clf.predict(X_test)\n",
    "  y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "  acc_score = accuracy_score(y_test, y_pred)\n",
    "  auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "  ### END CODE ###\n",
    "  print(\"Accuracy_score: {}, ROC_AUC_score: {}\".format(acc_score, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9a436542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Accuracy_score: 0.888, ROC_AUC_score: 0.5854582176218127\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree\")\n",
    "# Initialize your decision tree classifier\n",
    "### START CODE ###\n",
    "dt_clf = DecisionTreeClassifier(random_state=14)\n",
    "### END CODE ###\n",
    "\n",
    "# Fit the classifier on the training set\n",
    "dt_clf = fit_classifier(dt_clf, X_train, y_train)\n",
    "evaluate_classifier(dt_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d360a99"
   },
   "source": [
    "b. (2.5 pts) Create a [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) of 25 classifiers (i.e, n_estimators=25) with random_state=14. Please use Decision Tree Classifier with random_state=14 as the base classifier. Report accuracy_score and roc_auc_score on the test data for this emsemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "239c9c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging of Decesion Trees\n",
      "Accuracy_score: 0.9272, ROC_AUC_score: 0.7866043928300934\n"
     ]
    }
   ],
   "source": [
    "print(\"Bagging of Decesion Trees\")\n",
    "# Initialize your bagging classifier\n",
    "### START CODE ###\n",
    "bag_clf = BaggingClassifier(estimator=None,n_estimators=25, random_state=14)\n",
    "### END CODE ###\n",
    "\n",
    "bag_clf = fit_classifier(bag_clf, X_train, y_train)\n",
    "evaluate_classifier(bag_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34225ee3"
   },
   "source": [
    "c. (5 pts) In this question, you will fit a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model on the training data for this classification task.\n",
    "\n",
    "1. First, please find the best parameters (including *n_estimators*, *max_features* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the optimal parameters obtained by GridSearch.\n",
    "2. Fit a model using the best parameters, and report the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "R9E7sMGt2goM"
   },
   "outputs": [],
   "source": [
    "def grid_search_for_classifier(clf, param_grid, X_train, y_train):\n",
    "  # Grid search\n",
    "     grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "\n",
    "  # Conduct grid search using the training set (1 line of code only)\n",
    "  ### START CODE ###\n",
    "     grid_search.fit(X_train, y_train)\n",
    "  ### END CODE ###\n",
    "     print(grid_search.best_params_)\n",
    "\n",
    "  # Set the best paramters for your clf (1 line of code only)\n",
    "  ### START CODE ###\n",
    "   #clf.set_params(**grid_search.best_params_)\n",
    "     clf=grid_search.best_estimator_\n",
    "  ### END CODE ###\n",
    "     return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4UyA2_rB2gG9"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "  t0 = time()\n",
    "  # Fit your classifier on the training set\n",
    "  ### START CODE ###\n",
    "  clf.fit(X_train, y_train)\n",
    "  ### END CODE ###\n",
    "  print(\"training time\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "  t0 = time()\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(\"predict time\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "  print(\"Confusion matrix: \")\n",
    "  # Print the confusion matrix computed from the test set (1 line of code only)\n",
    "  ### START CODE ###\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "  ### END CODE ###\n",
    "\n",
    "\n",
    "  ### START CODE ###\n",
    "  y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "  acc_score = accuracy_score(y_test, y_pred)\n",
    "  auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "  ### END CODE ###\n",
    "\n",
    "  print(\"Accuracy: {}, AUC_ROC: {}\".format(acc_score, auc_score))\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "6bfb9542"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_features': 1, 'n_estimators': 100, 'random_state': 17}\n",
      "training time 0.178 s\n",
      "predict time 0.012 s\n",
      "Confusion matrix: \n",
      "[[1160    5]\n",
      " [  81    4]]\n",
      "Accuracy: 0.9312, AUC_ROC: 0.8378591264832114\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_features=1, random_state=17)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_features=1, random_state=17)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_features=1, random_state=17)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [1, 10, 50, 100],\n",
    "              \"max_features\": [1, 5, 10, \"auto\"],\n",
    "              \"criterion\": ['gini','entropy'],\n",
    "              \"random_state\": [17]}\n",
    "\n",
    "# Initialize your random forest classifier\n",
    "### START CODE ###\n",
    "rf_clf = RandomForestClassifier()\n",
    "### END CODE ###\n",
    "rf_clf = grid_search_for_classifier(rf_clf, param_grid, X_train, y_train)\n",
    "train_and_evaluate_classifier(rf_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "314c313b"
   },
   "source": [
    "d. (10 pts) This time, let us use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) for the same task. For AdaBoost and XGBoost, please respectively find the best parameters (including *n_estimators, learning_rate*); fit your model using the best parameters, and report the confusion matrix and roc_auc_score on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "3222ef25"
   },
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [10, 100],\n",
    "          \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "          \"random_state\": [17]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "bp4Ekjpy1cGw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 100, 'random_state': 17}\n",
      "training time 0.188 s\n",
      "predict time 0.008 s\n",
      "Confusion matrix: \n",
      "[[1153   12]\n",
      " [  72   13]]\n",
      "Accuracy: 0.9328, AUC_ROC: 0.8390254986114618\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=17)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=17)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=17)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your AdaBoost classifier\n",
    "### START CODE ###\n",
    "ab_clf = AdaBoostClassifier()\n",
    "### END CODE ###\n",
    "ab_clf = grid_search_for_classifier(ab_clf, param_grid, X_train, y_train)\n",
    "train_and_evaluate_classifier(ab_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Hv0p1mZh2CqA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 100, 'random_state': 17}\n",
      "training time 0.048 s\n",
      "predict time 0.002 s\n",
      "Confusion matrix: \n",
      "[[1163    2]\n",
      " [  84    1]]\n",
      "Accuracy: 0.9312, AUC_ROC: 0.8267962635698057\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=17, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=17, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=17, ...)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your XGBoost classifier\n",
    "### START CODE ###\n",
    "xgb_clf = xgboost.XGBClassifier()\n",
    "### END CODE ###\n",
    "xgb_clf = grid_search_for_classifier(xgb_clf, param_grid, X_train, y_train)\n",
    "train_and_evaluate_classifier(xgb_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6bc87cd"
   },
   "source": [
    "f. (5 pts) Compare the performance of decision tree from part a) with the ensemble methods. Briefly explain which of the three ensemble methods performed better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The accuracy from a single decision tree is around 88.8%.\n",
    "2. The accuracy from bagging model is 92.72%.\n",
    "3. The accuracy from randomforest model is 93.12%\n",
    "4. The accuracy from Adaboost model is 93.28% and accuracy from XGboost model is 93.12%\n",
    "\n",
    "Also, the AUC_ROC score of single decison tree is lesser than that of ensembles.\n",
    "\n",
    "We observe that the accuracy and AUC ROC scores of a single decision tree is lesser than the ensembles which aligns with the concept. Ensembles perfrom better than signle decision trees because they combine the predictions of multiple trees, which reduces the likelihood of overfitting and increases the model's ability to capture complex patterns in the data.\n",
    "\n",
    "Among the ensembles, we observe that Adaboost model outperforms Bagging and Randomforest models. AdaBoost places more weight on the training instances that were previously misclassified.This can lead to a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZXTR1tM7yBp"
   },
   "source": [
    "# Q3: CatBoost (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zcExque_yso"
   },
   "source": [
    "In this question you will learn about a boosting algorithm known as **CatBoost**. Please go through the two videos specified below to get a better understanding of the CatBoost algorithm and answer the questions that follow.\n",
    "\n",
    "[Part-1](https://www.youtube.com/watch?v=KXOTSkPL2X4&ab_channel=StatQuestwithJoshStarmer)\n",
    "[Part - 2](https://www.youtube.com/watch?v=3Bg2XRFOTzg&t=242s&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "\n",
    "\n",
    "a. **(5 points)** Briefly explain Ordered Target Encoding. What challenge does it try to address?\n",
    "\n",
    "b. **(5 points)** Briefly describe the main advantages and disadvantages of CatBoost as compared to XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "#### Ordered Target Encoding\n",
    "Ordered Target Encoding is a technique used in machine learning to encode categorical variables to numericals by ranking them in relation to the target variable.In simpler terms, CatBoost calculates a statistic for each category by updating the average of the target variable for that category, based on the data points that come before the current row in the dataset. This means that, for any given row, CatBoost considers the historical data for that category to determine its statistic.\n",
    "\n",
    "Ordered Target encoding helps avoid information leakage from the target variable because it dynamically updates the mean based on the order of the data.\n",
    "\n",
    "Ordered Target Encoding can also help prevent overfitting by encoding categorical variables to numerical values such that it reflects their relationship with the target variable and reduces the risk of introducing noise into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "#### Advantages of CatBoost over XGBoost:\n",
    "\n",
    "1. CatBoost can naturally handle categorical data without the need for encoding, making it easier to work with datasets that have categorical variables. In contrast, XGBoost requires preprocessing of categorical features such as one hot encoding.\n",
    "\n",
    "2. CatBoost is designed to be less prone to overfitting compared to XGBoost.\n",
    "\n",
    "3. CatBoost can efficiently utilize GPUs for training, which can significantly speed up the training process, especially for categorical datasets. XGBoost also supports GPU acceleration, but CatBoost's implementation is often easier to work with.\n",
    "\n",
    "#### Disadvantages of CatBoost over XGBoost:\n",
    "\n",
    "1. CatBoost is slower to train compared to XGBoost,especially for large datasets. XGBoost on the other hand is known for its training speed.\n",
    "\n",
    "2. CatBoost is less flexibility in hyperparameter tuning compared to XGBoost. XGBoost offers a wider range of hyperparameters for fine-tuning and customization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTFRT23o76A2"
   },
   "source": [
    "# Q4: Convolutional Neural Network (20 points)\n",
    "In this question, we will continue our exercise on the SVHN classification task from the previous homework, but this time we will be using Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "lW0odgzmXUU_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmolagrawal/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Dr0KdJ2LrCCV"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "oBCdEj-_0tkS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 182040794/182040794 [00:08<00:00, 22579997.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 64275384/64275384 [00:05<00:00, 12688898.16it/s]\n"
     ]
    }
   ],
   "source": [
    "transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "        ])\n",
    "\n",
    "train_dataset = torchvision.datasets.SVHN(root='.', split='train', transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.SVHN(root='.', split='test', transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "iLn4YKK90vjo"
   },
   "outputs": [],
   "source": [
    "train_num = int(len(train_dataset) * 0.8)\n",
    "val_num = len(train_dataset) - train_num\n",
    "# Randomly split the training dataset into training dataset and validation dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_num, val_num])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFUXsbsqjoCw"
   },
   "source": [
    "a. (10 points) Build a convolutional neural network with the following sequential configuration. If not specified, please use the default setting of torch.nn.Conv2d. The output of the convolution layers will be fed into a fully-connected MLP. Then train the model with Adam optimizer (lr=1e-3) for 10 epochs. You should be able to achieve test accuracy of over 85%.\n",
    "\n",
    "\n",
    "\n",
    "> Layer 1\n",
    "*   2d convolution (# input channel=3, # output channel=16, kernel size=3, padding=1)\n",
    "*   2d batch normalization\n",
    "*   Relu activation\n",
    "\n",
    "> Pool 1\n",
    "*   2d max pooling (kernel size=2)\n",
    "\n",
    "> Layer 2\n",
    "*   2d convolution (# output channel=16, kernel size=3, padding=1)\n",
    "*   2d batch normalization\n",
    "*   Relu activation\n",
    "\n",
    "> Pool 2\n",
    "*   2d max pooling (kernel size=2)\n",
    "\n",
    "> Layer 3\n",
    "*   2d convolution (# output channel=32, kernel size=3, padding=1)\n",
    "*   2d batch normalization\n",
    "*   Relu activation\n",
    "\n",
    "> Pool 3\n",
    "*   2d max pooling (kernel size=2)\n",
    "\n",
    "References:\n",
    "\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "M8OcO88V01Rr"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, pool=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.pool = pool\n",
    "\n",
    "        # Create convolutional layers\n",
    "        ### START CODE ###\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1),nn.BatchNorm2d(16),nn.ReLU())\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(16, 16, kernel_size=3, padding=1),nn.BatchNorm2d(16),nn.ReLU())\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, padding=1),nn.BatchNorm2d(32),nn.ReLU())\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Create fully connected layers (nn.Linear)\n",
    "        if self.pool:\n",
    "            self.mlp1 = nn.Linear(32*4*4, 50)\n",
    "        else:\n",
    "            self.mlp1 = nn.Linear(32*32*32, 50)\n",
    "\n",
    "        self.mlp2 = nn.Linear(50, 50)\n",
    "        self.mlp3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        if self.pool:\n",
    "            x = self.pool1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        if self.pool:\n",
    "            x = self.pool2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        if self.pool:\n",
    "            x = self.pool3(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = self.mlp3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Ygrxekov097w"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "    for data, target in tqdm(loader):\n",
    "        out = model(data)\n",
    "        # Calculate loss based on model output and target\n",
    "        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n",
    "\n",
    "        # Use the optimizer to perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = len(target)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_num += batch_size\n",
    "    avg_loss = total_loss / total_num\n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    for data, target in tqdm(loader):\n",
    "        out = model(data)\n",
    "        # Calculate loss based on model output and target\n",
    "        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n",
    "\n",
    "        # Get model's prediction\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct = accuracy_score(target, pred, normalize=False)\n",
    "\n",
    "        total_correct += correct\n",
    "        batch_size = len(target)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_num += batch_size\n",
    "    avg_loss = total_loss / total_num\n",
    "    acc = total_correct / total_num\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "aET_5IqXDsC1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:23<00:00,  9.54it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.2207654907499785 Val Loss: 0.6438848191423231 Val Acc: 0.8101965601965602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:24<00:00,  9.53it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.561296421849334 Val Loss: 0.5030663549379348 Val Acc: 0.8526481026481026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:24<00:00,  9.53it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.45946389047849934 Val Loss: 0.46861606540167033 Val Acc: 0.861998361998362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:23<00:00,  9.55it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.4115212148513905 Val Loss: 0.4214219909131836 Val Acc: 0.875034125034125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:24<00:00,  9.54it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.3756593310632617 Val Loss: 0.4116648929735171 Val Acc: 0.8774911274911275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:23<00:00,  9.56it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.3523911556711621 Val Loss: 0.4073008431898191 Val Acc: 0.8757848757848757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:24<00:00,  9.52it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.3327306432164007 Val Loss: 0.3912297532007143 Val Acc: 0.8847256347256347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:23<00:00,  9.55it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.317665394251994 Val Loss: 0.3702919187887165 Val Acc: 0.8901173901173901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:23<00:00,  9.55it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.30021996573980386 Val Loss: 0.3743383329348456 Val Acc: 0.8880698880698881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:24<00:00,  9.53it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:02<00:00, 24.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.2900898263688564 Val Loss: 0.3637780602345075 Val Acc: 0.8948266448266449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = CNN(pool=True)\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "best_acc = -np.inf\n",
    "epochs = 10\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(model1, train_loader, optimizer)\n",
    "    val_loss, val_acc = eval(model1, val_loader)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model1 = model1\n",
    "    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "A7Za_DdlDwJX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 102/102 [00:04<00:00, 24.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = eval(best_model1, test_loader)\n",
    "print(f\"Test accuracy: {np.round(test_acc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLAnNsjsnRpK"
   },
   "source": [
    "b. (5 points) Use torch-summary to print a summary of the model. The number of parameters should be less than the one of the MLP we trained in the previous homework. Why does it have less number of parameters but have higher accuracy?\n",
    "\n",
    "Reference\n",
    "*   https://pypi.org/project/torch-summary/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "tkuRsoNxR8dX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 16, 32, 32]          --\n",
      "|    └─Conv2d: 2-1                       [-1, 16, 32, 32]          448\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 16, 32, 32]          32\n",
      "|    └─ReLU: 2-3                         [-1, 16, 32, 32]          --\n",
      "├─MaxPool2d: 1-2                         [-1, 16, 16, 16]          --\n",
      "├─Sequential: 1-3                        [-1, 16, 16, 16]          --\n",
      "|    └─Conv2d: 2-4                       [-1, 16, 16, 16]          2,320\n",
      "|    └─BatchNorm2d: 2-5                  [-1, 16, 16, 16]          32\n",
      "|    └─ReLU: 2-6                         [-1, 16, 16, 16]          --\n",
      "├─MaxPool2d: 1-4                         [-1, 16, 8, 8]            --\n",
      "├─Sequential: 1-5                        [-1, 32, 8, 8]            --\n",
      "|    └─Conv2d: 2-7                       [-1, 32, 8, 8]            4,640\n",
      "|    └─BatchNorm2d: 2-8                  [-1, 32, 8, 8]            64\n",
      "|    └─ReLU: 2-9                         [-1, 32, 8, 8]            --\n",
      "├─MaxPool2d: 1-6                         [-1, 32, 4, 4]            --\n",
      "├─Linear: 1-7                            [-1, 50]                  25,650\n",
      "├─Linear: 1-8                            [-1, 50]                  2,550\n",
      "├─Linear: 1-9                            [-1, 10]                  510\n",
      "==========================================================================================\n",
      "Total params: 36,246\n",
      "Trainable params: 36,246\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.36\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 0.49\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 16, 32, 32]          --\n",
       "|    └─Conv2d: 2-1                       [-1, 16, 32, 32]          448\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 16, 32, 32]          32\n",
       "|    └─ReLU: 2-3                         [-1, 16, 32, 32]          --\n",
       "├─MaxPool2d: 1-2                         [-1, 16, 16, 16]          --\n",
       "├─Sequential: 1-3                        [-1, 16, 16, 16]          --\n",
       "|    └─Conv2d: 2-4                       [-1, 16, 16, 16]          2,320\n",
       "|    └─BatchNorm2d: 2-5                  [-1, 16, 16, 16]          32\n",
       "|    └─ReLU: 2-6                         [-1, 16, 16, 16]          --\n",
       "├─MaxPool2d: 1-4                         [-1, 16, 8, 8]            --\n",
       "├─Sequential: 1-5                        [-1, 32, 8, 8]            --\n",
       "|    └─Conv2d: 2-7                       [-1, 32, 8, 8]            4,640\n",
       "|    └─BatchNorm2d: 2-8                  [-1, 32, 8, 8]            64\n",
       "|    └─ReLU: 2-9                         [-1, 32, 8, 8]            --\n",
       "├─MaxPool2d: 1-6                         [-1, 32, 4, 4]            --\n",
       "├─Linear: 1-7                            [-1, 50]                  25,650\n",
       "├─Linear: 1-8                            [-1, 50]                  2,550\n",
       "├─Linear: 1-9                            [-1, 10]                  510\n",
       "==========================================================================================\n",
       "Total params: 36,246\n",
       "Trainable params: 36,246\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.36\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.34\n",
       "Params size (MB): 0.14\n",
       "Estimated Total Size (MB): 0.49\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Print the model summary\n",
    "summary(model, (3, 32, 32))  # Assuming input size of (3, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs use parameter sharing and local connectivity, which significantly reduce the number of parameters while efficiently capturing local and hierarchical patterns in images. This makes them more suitable for handling the high-dimensional data typical of images.\n",
    "\n",
    "Also, pooling layers in CNNs provide the network with spatial invariance to input translation, making them better at recognizing objects in images regardless of their position. MLPs lack this mechanism, making them less effective for tasks where object location can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eb6rNHsobVb"
   },
   "source": [
    "c. (5 points) Train another CNN with the pool option set to False. What are the differences in terms of accuracy or computation caused by disabling max pooling? What are the effects of pooling operations in CNNs? (This might take some time. Watch a TV show while you're waiting for the results..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "1m-9yG9VAbgE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.25it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.1317163487961954 Val Loss: 0.5977354190436146 Val Acc: 0.8203658203658204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.24it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.4888308106739258 Val Loss: 0.4922866175287316 Val Acc: 0.8506006006006006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.25it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.4088029093149833 Val Loss: 0.435110639433961 Val Acc: 0.8704613704613705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.24it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.35486409986357814 Val Loss: 0.43406999131515644 Val Acc: 0.8707343707343708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.24it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:05<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.31420408161403923 Val Loss: 0.41567200717218517 Val Acc: 0.8761261261261262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.24it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.2826858134971891 Val Loss: 0.4387009419844576 Val Acc: 0.8697106197106197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.24it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.25789678148641165 Val Loss: 0.41514690935920895 Val Acc: 0.8818591318591319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:11<00:00,  3.20it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:05<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.22629386106353908 Val Loss: 0.4354146088679697 Val Acc: 0.8813131313131313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:11<00:00,  3.21it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.20229596501044186 Val Loss: 0.4676882262002761 Val Acc: 0.8741468741468742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [01:10<00:00,  3.24it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:04<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.18000401606251382 Val Loss: 0.4502070361543411 Val Acc: 0.8757848757848757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = CNN(pool=False)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "best_acc = -np.inf\n",
    "epochs = 10\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(model2, train_loader, optimizer)\n",
    "    val_loss, val_acc = eval(model2, val_loader)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model2 = model2\n",
    "    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "13gIhnYVAbgQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 102/102 [00:08<00:00, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607867240319607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = eval(best_model2, test_loader)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disabling max pooling in CNNs increases computational load due to larger spatial dimensions being processed and can reduce accuracy by making the network more sensitive to the exact location of features.\n",
    "\n",
    "The accuracies we obtained aligns with this concept. With max pooling, the test accuracy is 89% but without max pooling, the accuracy drops to around 86%. This indicates that enabling pooling helps the model to generalise better and perform better on the unseen data.\n",
    "\n",
    "Effect of Pooling in CNNs:\n",
    "\n",
    "1. Pooling reduce the spatial dimensions of the feature maps, which cuts down the computational requirements and the number of parameters in the network. This helps in reducing overfitting.\n",
    "\n",
    "2. Pooling also makes CNN less sensitive to the exact location of features in the image, thus enhancing the model's ability to recognize patterns irrespective of their position."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
